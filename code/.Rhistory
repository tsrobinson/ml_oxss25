s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
lm(Y ~ treat_, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash","treat_text"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results)
t1_taus <- c("cash" = -0.05, "text" = 0.05, "control" = 0)
power_results_post <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y_0 = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(Y_1 = rbinom(nrow(.), size = 1, prob = max(p + t1_taus[treat_], 0))) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
select(-s_fe, v_fe, p) %>%
pivot_longer(all_of(c("Y_0", "Y_1")),
names_prefix = "Y_",
names_to = "t",
values_to = "Y") %>%
lm(Y ~ treat_*t, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash:t1","treat_text:t1"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results_post)
## VERSION 2
library(tidyverse)
set.seed(89)
#####################
villages <- 75
subj <- 65
power_results <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
lm(Y ~ treat_, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash","treat_text"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results)
t1_taus <- c("cash" = -0.05, "text" = 0.05, "control" = 0)
power_results_post <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y_0 = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(Y_1 = rbinom(nrow(.), size = 1, prob = max(p + t1_taus[treat_], 0))) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
select(-s_fe, v_fe, p) %>%
pivot_longer(all_of(c("Y_0", "Y_1")),
names_prefix = "Y_",
names_to = "t",
values_to = "Y") %>%
lm(Y ~ treat_*t, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash:t1","treat_text:t1"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results_post)
## VERSION 2
library(tidyverse)
set.seed(89)
#####################
villages <- 75
subj <- 70
power_results <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
lm(Y ~ treat_, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash","treat_text"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results)
t1_taus <- c("cash" = -0.05, "text" = 0.05, "control" = 0)
power_results_post <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y_0 = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(Y_1 = rbinom(nrow(.), size = 1, prob = max(p + t1_taus[treat_], 0))) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
select(-s_fe, v_fe, p) %>%
pivot_longer(all_of(c("Y_0", "Y_1")),
names_prefix = "Y_",
names_to = "t",
values_to = "Y") %>%
lm(Y ~ treat_*t, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash:t1","treat_text:t1"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results_post)
## VERSION 2
library(tidyverse)
set.seed(89)
#####################
villages <- 75
subj <- 75
power_results <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
lm(Y ~ treat_, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash","treat_text"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results)
t1_taus <- c("cash" = -0.05, "text" = 0.05, "control" = 0)
power_results_post <- lapply(1:1000, function (x) {
data.frame(
village = rep(paste0("v",1:villages), each = subj),
treat_ = rep(sample(c("cash", "text", "control"), size = villages, replace = TRUE), each = subj),
s_fe = rnorm(n = subj*villages, mean = 0, sd = 0.01),
v_fe = rep(rnorm(n = villages, mean = 0, sd = 0.01), each = subj)
) %>%
mutate(p = 0.1 + ifelse(treat_ %in% c("cash","text"), 0.05, 0) + v_fe + s_fe) %>%
mutate(Y_0 = rbinom(nrow(.), size = 1, prob = p)) %>%
mutate(Y_1 = rbinom(nrow(.), size = 1, prob = max(p + t1_taus[treat_], 0))) %>%
mutate(treat_ = factor(treat_, levels = c("control","cash","text"))) %>%
select(-s_fe, v_fe, p) %>%
pivot_longer(all_of(c("Y_0", "Y_1")),
names_prefix = "Y_",
names_to = "t",
values_to = "Y") %>%
lm(Y ~ treat_*t, .) %>%
broom::tidy() %>%
filter(term %in% c("treat_cash:t1","treat_text:t1"))
}
) %>%
do.call("rbind",.) %>%
group_by(term) %>%
summarise(power = mean(p.value < 0.05),
"E[beta]" = mean(estimate))
print(power_results_post)
75*75
library(rpart?)
library(rpart)
?rpart
unlink("~/Dept of Methodology Dropbox/Dept of Methodology Team Folder/Teaching and Assessment/Courses/MY474/202324/seminars/seminar3_cache", recursive = TRUE)
library(haven)
library(lfe)
library(glmnet)
?glmnet
library(haven)
library(lfe)
library(glmnet)
emw <- read_dta("../data/efmw_replication.dta")
emw$dist <- log(1 + (1 / (emw$dist_coast)))
emw$distwremit <- log(1 + ( (emw$richremit / 1000000) * (emw$dist)))
# subset columns
emw <- emw[, c("Protest", "remit", "dict", "l1gdp", "l1pop", "l1nbr5", "l12gr",
"l1migr", "elec3", "cowcode", "period", "distwremit",
"caseid", "year")]
emw <- na.omit(emw)
controls <- c("l1gdp", "l1pop", "l1nbr5", "l12gr", "l1migr", "elec3")
View(emw)
contr.sum
names(contr.list) <- c("factor(period)","factor(cowcode)")
# Generate fixed effects (don't worry too much about this)
contr.list <- list(contr.sum, contr.sum)
names(contr.list) <- c("factor(period)","factor(cowcode)")
mod_mat <- model.matrix(~factor(period)+factor(cowcode),data=emw,contrasts.arg=contr.list)[,-1]
View(mod_mat)
X <- as.matrix(cbind(emw[,controls],mod_mat))
V <- emw$dict
emw$dict
VX <- as.matrix(V*X)
D <- emw$remit
D
DV <- D*V
DV
Y <- emw$Protest
Y
lasso_selector <- function(LHS, RHS) {
# Estimate the LASSO model
# NOTE: we are going to assume a lambda value here
# We will discuss how to choose this value in a principled way tomorrow
lasso <- glmnet(x=RHS, y=LHS, alpha=1, lambda = 0.002)
# Find non-zero coefficients by their index position
coef_index <- which(coef(lasso) != 0) - 1
return(coef_index)
}
RHS_matrix <- as.matrix(cbind(V = V,X,VX))
View(RHS_matrix)
# Optional but useful to keep track of names
colnames(RHS_matrix) <- c("V", colnames(X),
paste0("V_",colnames(X)))
View(RHS_matrix)
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix)
Y_lasso
D_lasso
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix)
DV_lasso
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
selected_columns
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix)
# we don't want to select the same column more than once, so use unique()
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
selected_columns
ds_matrix <- as.data.frame(cbind(Protest=Y,
remit=D,
remit_dict=DV,
RHS_matrix[,selected_columns]))
ds_model <- glm("Protest~.", data = ds_matrix)
naive_model <- glm(paste0(c("Protest ~ remit*dict",
controls,
"as.factor(period) + as.factor(cowcode)"),
collapse = " + "),
data = cbind(emw, remit_dict = DV))
message("Naive model:")
summary(naive_model)$coefficients[c("remit","remit:dict"),]
message("DS model:")
summary(ds_model)$coefficients[c("remit","remit_dict"),]
lasso_selector <- function(LHS, RHS, l_val) {
# Estimate the LASSO model
# NOTE: we are going to assume a lambda value here
# We will discuss how to choose this value in a principled way tomorrow
lasso <- glmnet(x=RHS, y=LHS, alpha=1, lambda = l_val)
# Find non-zero coefficients by their index position
coef_index <- which(coef(lasso) != 0) - 1
return(coef_index)
}
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, l_val = 0.002)
Y_lasso
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, l_val = 0.02)
Y_lasso
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, l_val = 0.2)
Y_lasso
cv_mod <- cv.glmnet(x=RHS, y=LHS, alpha=1)
cv_mod <- cv.glmnet(x=RHS_matrix, y=Y, alpha=1)
View(cv_mod)
cv_mod$lambda.min
best_lambda <- cv_mod$lambda.min
cv_mod <- cv.glmnet(x=RHS_matrix, y=Y, alpha=1)
best_lambda <- cv_mod$lambda.min
final_mod <- glmnet(x=RHS_matrix, y=Y, alpha=1, lambda = best_lambda)
coef(final_mod)
cv_mod <- cv.glmnet(x=RHS_matrix, y=Y, alpha=0)
best_lambda <- cv_mod$lambda.min
final_mod <- glmnet(x=RHS_matrix, y=Y, alpha=0, lambda = best_lambda)
coef(final_mod)
cv_mod <- cv.glmnet(x=RHS_matrix, y=Y, alpha=0.3)
cv_mod <- cv.glmnet(x=RHS_matrix, y=Y, alpha=0.3)
best_lambda <- cv_mod$lambda.min
final_mod <- glmnet(x=RHS_matrix, y=Y, alpha=0.3, lambda = best_lambda)
coef(final_mod)
library(tensorflow)
library(keras)
library(tidyverse)
library(recipes)
## Read in the data
adult <- read_csv("https://raw.githubusercontent.com/MIDASverse/MIDASpy/master/Examples/adult_data.csv") %>%
drop_na() %>% # Not good practise!! (for the sake of demonstration only)
select(-1) # remove the first column as it's just the row indices
View(adult)
# Break up our data into train and test
train_index <- sample(1:nrow(adult), 0.666*nrow(adult), replace = FALSE)
adult_train <- adult[train_index,]
adult_test <- adult[setdiff(1:nrow(adult), train_index),]
y_train <- ifelse(adult_train$class_labels == ">50K",1,0)
y_test <- ifelse(adult_test$class_labels == ">50K",1,0)
length(unique(adult$native_country))
# Construct a "recipe"
rec_obj <- recipe(class_labels ~ ., data = adult) %>%
step_dummy(all_nominal(), -all_outcomes()) %>% # One-hot encode columns
step_center(all_predictors(), -all_outcomes()) %>% # Centre all predictors on 0
step_scale(all_predictors(), -all_outcomes()) %>% # Scale all predictors with sd=1
prep(data = adult)
View(rec_obj)
x_train <- bake(rec_obj, new_data = adult_train) %>% select(-class_labels)
x_test  <- bake(rec_obj, new_data = adult_test) %>% select(-class_labels)
View(x_train)
ncol(x_train)
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
?layer_dense
model <- keras_model_sequential() %>%
layer_dense(object = ., units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(object = ., units = 16, activation = 'relu') %>%
layer_dense(object = ., units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
t.robinson7@lse.ac.uk
library(tensorflow)
library(keras)
library(tidyverse)
library(recipes)
## Read in the data
adult <- read_csv("https://raw.githubusercontent.com/MIDASverse/MIDASpy/master/Examples/adult_data.csv") %>%
drop_na() %>% # Not good practise!! (for the sake of demonstration only)
select(-1) # remove the first column as it's just the row indices
# Break up our data into train and test
train_index <- sample(1:nrow(adult), 0.666*nrow(adult), replace = FALSE)
adult_train <- adult[train_index,]
adult_test <- adult[setdiff(1:nrow(adult), train_index),]
y_train <- ifelse(adult_train$class_labels == ">50K",1,0)
y_test <- ifelse(adult_test$class_labels == ">50K",1,0)
# Construct a "recipe"
rec_obj <- recipe(class_labels ~ ., data = adult) %>%
step_dummy(all_nominal(), -all_outcomes()) %>% # One-hot encode columns
step_center(all_predictors(), -all_outcomes()) %>% # Centre all predictors on 0
step_scale(all_predictors(), -all_outcomes()) %>% # Scale all predictors with sd=1
prep(data = adult)
x_train <- bake(rec_obj, new_data = adult_train) %>% select(-class_labels)
x_test  <- bake(rec_obj, new_data = adult_test) %>% select(-class_labels)
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
model_w_dropout <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
optimizer = 'sgd', # Stochastic gradient descent -- a variation of what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
uninstall.packages("keras")
remove.packages("keras")
install.packages("keras3")
keras3::install_keras()
install.packages("remotes")
remotes::install_github("rstudio/tensorflow")
remotes::install_github("rstudio/tensorflow")
install.packages("reticulate")
tensorflow::install_tensorflow(envname = "r-reticulate")
install.packages("keras3")
keras3::install_keras()
?keras3::install_keras
## Set up commands -- follow this to set up on your own machine
# install.packages("remotes")
# remotes::install_github("rstudio/tensorflow")
# install.packages("reticulate")
# tensorflow::install_tensorflow(envname = "r-reticulate")
# install.packages("keras3")
# keras3::install_keras("r-reticulate")
# install.packages("recipes")
```
keras3::install_keras("r-reticulate")
install.packages("recipes")
library(tensorflow)
library(keras)
library(tensorflow)
library(keras3)
library(tidyverse)
library(recipes)
## Read in the data
adult <- read_csv("https://raw.githubusercontent.com/MIDASverse/MIDASpy/master/Examples/adult_data.csv") %>%
drop_na() %>% # Not good practise!! (for the sake of demonstration only)
select(-1) # remove the first column as it's just the row indices
# Break up our data into train and test
train_index <- sample(1:nrow(adult), 0.666*nrow(adult), replace = FALSE)
adult_train <- adult[train_index,]
adult_test <- adult[setdiff(1:nrow(adult), train_index),]
y_train <- ifelse(adult_train$class_labels == ">50K",1,0)
y_test <- ifelse(adult_test$class_labels == ">50K",1,0)
# Construct a "recipe"
rec_obj <- recipe(class_labels ~ ., data = adult) %>%
step_dummy(all_nominal(), -all_outcomes()) %>% # One-hot encode columns
step_center(all_predictors(), -all_outcomes()) %>% # Centre all predictors on 0
step_scale(all_predictors(), -all_outcomes()) %>% # Scale all predictors with sd=1
prep(data = adult)
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
history <- fit(
object = model,
x = as.matrix(x_train),
y = y_train,
batch_size = 50,
epochs = 50,
validation_split = 0.30
)
model_w_dropout <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
optimizer = 'sgd', # Stochastic gradient descent -- a variation of what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
history2 <- fit(
object = model_w_dropout,
x = as.matrix(x_train),
y = y_train,
batch_size = 50,
epochs = 50,
validation_split = 0.30
)
getwd
getwd()
library(tidyverse)
set.seed(89)
# YouGov data for training model
load("ylong.RData")
colnames(ylong)
covars_keep <- c("age","gender","education","religion","urban",
"inc_hhold","vote_2024","vote_intent","region",
"social_grade")
